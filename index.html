<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>My Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="My Blog">
<meta property="og:url" content="https://weij0626.github.io/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:locale" content="zh_TW">
<meta property="article:author" content="Jeff Chung">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="My Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">My Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">紀錄論文心得</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜尋"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://weij0626.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/02/hello-world/" class="article-date">
  <time datetime="2020-07-01T16:10:00.000Z" itemprop="datePublished">2020-07-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/02/hello-world/">Learning to Restore Low-Light Images via Decomposition-and-Enhancement</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Authors-Ke-Xu-Xin-Yang-Baocai-Yin-Rynson-W-H-Lau-CVPR2020"><a href="#Authors-Ke-Xu-Xin-Yang-Baocai-Yin-Rynson-W-H-Lau-CVPR2020" class="headerlink" title="Authors : Ke Xu, Xin Yang, Baocai Yin, Rynson W.H. Lau, CVPR2020"></a>Authors : Ke Xu, Xin Yang, Baocai Yin, Rynson W.H. Lau, CVPR2020</h4><h4 id="Source-paper"><a href="#Source-paper" class="headerlink" title="Source : paper"></a>Source : <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_Learning_to_Restore_Low-Light_Images_via_Decomposition-and-Enhancement_CVPR_2020_paper.html" target="_blank" rel="noopener">paper</a></h4><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>對於低亮度圖像的圖像增強，我們通常需要解決兩大問題。第一即為提高亮度(pixel intensity)，第二則是解決噪音(noise)問題。現今許多的低亮度圖像增強方法都是學習在幾乎沒有噪音(noise-negligible)的資料集上，而這種方法則會依賴於使用者能拍出低噪音的低亮度影像作為輸入，當然這顯然是不現實的。作者觀察到噪音在不同頻率下有著不同程度的對比，在低頻的狀況下我們能夠更簡單的來偵測到噪音，因此這篇論文提出了frequency-based decomposition-and-enhancement model。這個模型會先從輸入圖片中取出低頻部分，把低頻部份進行去噪和增強過後，再利用低頻部份去復原高頻的細節。</p>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>在介紹本篇dataset之前我們先簡單講一下SID dataset，SID dataset包含5094張短曝光的raw images，每張都對應到一張長曝光的參考圖。Ps :多張短曝光圖像有可能會對應到同一張長曝光參考圖。<br>因為線性的raw image data和非線性的sRGB data相比，在噪音和圖像強度方面會有很大的不同，這就造成了訓練在SID dataset上的模型不能直接應用在sRGB圖像上。<br>因此這篇論文以SID dataset為基礎，建構了一個新的sRGB dataset。在從raw data轉到sRGB的過程，作者對曝光補償(exposure compensation)、白平衡(white balance)、de-linearization(去線性化)做了一些操作來模擬現實中可能由不同相機所拍成的noisy low-light sRGB images。</p>
<h4 id="曝光補償"><a href="#曝光補償" class="headerlink" title="曝光補償"></a>曝光補償</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">隨機從[0EV, 2EV]中以0.5EV作為間隔來取樣，以模仿曝光時間的多樣性。</span><br></pre></td></tr></table></figure>
<h4 id="白平衡"><a href="#白平衡" class="headerlink" title="白平衡"></a>白平衡</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">隨機從[2100K, 4000K]中選取色溫作為標準</span><br></pre></td></tr></table></figure>
<h4 id="去線性化"><a href="#去線性化" class="headerlink" title="去線性化"></a>去線性化</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因為不同相機所使用的camera response <span class="keyword">function</span> 皆不一樣且很難去做逆向工程，所以這邊直接用gamma <span class="keyword">function</span>來做為de-linearization <span class="keyword">function</span>。</span><br></pre></td></tr></table></figure>

<p>作者最後得到5394個image pair，其中4198對作為訓練資料，1196對作為測試資料。</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/model.png" alt="Model 架構圖"><br>這個模型主要分成兩個階段，第一階段先去學低頻的image enhancement function C(.)，然後再去學amplification function A(.)來做color recovery，這個階段將global information(e.g. illumination)和local information(e.g. color)分開來學，會使得訓練更有效率。給予一個低亮度的圖像I，這第一階段的enhancement可以被寫作是：<br><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/model2.png" alt=""><br>要注意的是，A在這邊是代表我們從C所估計得到的amplification map，而A會跟global ratio α作呼應，而非Retinex-based methods裡面的illumination map。 (Question : global ratio怎麼得來的?)<br>第二階段則是根據第一階段所得到的I^a來學習高頻的detail enhancement function D(.)，這裡採用的是residual learning，最後我們能得到下列式子：<br><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/model3.png" alt=""><br><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/model4.png" alt="各階段結果"></p>
<h2 id="Novel-Modules"><a href="#Novel-Modules" class="headerlink" title="Novel Modules"></a>Novel Modules</h2><p>接下來則是這篇論文所提出的兩個新模塊。</p>
<h3 id="ACE-Module-–-Attention-to-Context-Encoding"><a href="#ACE-Module-–-Attention-to-Context-Encoding" class="headerlink" title="ACE Module – Attention to Context Encoding"></a>ACE Module – Attention to Context Encoding</h3><p><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/module1.png" alt="ACE Module"><br>這個模塊的主要目的是去學習frequency-aware features for image decomposition，透過用兩個不同kernel和dilation的convolution相減來得到contrast-aware attention map Ca。<br><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/module2.png" alt=""><br>由於我們是要先從低頻的開始作，所以我們令Ca’ = 1-Ca且Xc =  Ca’ ∙ Xin，Xc就會是整個圖像的低頻部分，接著對Xc作shrink<br>來降低記憶體的用量後，讓Xc分別經過f、g、h三個function，這邊的f、g、h是各自代表不同的convolution、reshape、matrix transpose，讓g(Xc^T)乘h(Xc)來得到pixel affinity table M, M ∈ R^(H’W’x H’W’)。再讓f(Xc^T )乘M來得到non-locally enhanced features，這邊的non-locally是指每一個pixel都考慮到了它跟其他所有pixel之間的關係。最後再讓Xc加上enhanced features便能得到結果。高頻的話就是一樣作法，唯一不同點就在於這邊直接用Ca而非Ca’。</p>
<p><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/module3.png" alt=""></p>
<p>由上圖可以很清楚地看出Ca’確實對應低頻的背景，而Ca則對應於較為高頻的前景細節。</p>
<h3 id="CDT-Module-–-Cross-Domain-Transformation"><a href="#CDT-Module-–-Cross-Domain-Transformation" class="headerlink" title="CDT Module – Cross Domain Transformation"></a>CDT Module – Cross Domain Transformation</h3><p><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/module4.png" alt="CDT Module"><br>CDT模塊的主要目的則是去增加receptive fields以及縮小low light domain跟enhanced domain之間的差異。這邊利用Unet的概念，只是將skip connection所要concatenate的encoded layer改成經過CDT處理後的layer。Encoded layer和decoded layer concatenate之後，為了縮小差距，這邊會多乘上一個channel-wise scaling vector v。(這邊他沒細講scaling vector求法，我猜是乘上一個平均值來讓低於最大值平均值的其他channel能和最大值channel差不多)，最後一層的scaling vector v應該就對應於global ratio α。</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/experiment1.png" alt="Experiment1"><br><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/experiment2.png" alt="Experiment2"><br>這邊可以看到WVM和DeepUPE都enhance失敗，因為他們都是Retinex-based的方法，當輸入太過於low light的時候，他們沒辦法正確的去decomposition。LIME和CAPE能enhance images，但他們同時也放大了noise。SID這邊他們則是重新訓練在sRGB上，可以看到SID傾向於去除噪音和細節，造成圖片模糊，而我們的方法能夠在去除噪音的同時保留住細節。<br><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/experiment3.png" alt="Experiment3"></p>
<h2 id="Failure-cases"><a href="#Failure-cases" class="headerlink" title="Failure cases"></a>Failure cases</h2><p><img src="https://raw.githubusercontent.com/weij0626/weij0626.github.io/master/0702/failure1.png" alt="Failure case"><br>這個方法的限制在於場景如果有small object的話會比較難復原，因為ACE module用到類似self-attention和long-range relation的概念，意及網路不會特別著重於空間上的相關性，因此如果物件太小的話，網路比較難去萃取出有意義的contextual information。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://weij0626.github.io/2020/07/02/hello-world/" data-id="ckc3mqka30000qkum6gr70pde" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">彙整</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/07/02/hello-world/">Learning to Restore Low-Light Images via Decomposition-and-Enhancement</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Jeff Chung<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>